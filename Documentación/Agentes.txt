Este archivo contienen cada uno de los modelos creados en la parte de experimentación. Cada modelo se configura de diferente manera para que se pueda modelar un
comportamiento diferente. En general todos los modelos comparten varias caracteristicas.

Objetivo: El agente debe ser capaz de jugar BrainFighter de modo que represente para los jugadores reales una dificultad aceptable (ni muy fácil ni muy dificil)
Entorno: Escenario de la ciudad donde dos Agentes se enfrentan hasta que uno de los dos muera

Agente Alpha (Modelo Alpha):

Este modelo es la primer versión, le llamé así debido a que en el momento en el que se realizo el entrenamiento, el juego no habia pasado por el proceso de control de calidad
y tenía bugs visuales además de que había acciones muy injustas que el agente a la hora de entrenarse me dio a conocer.

Caracteristicas.
    Observaciones:
        -> Distancia del enemigo (flotante, normalizado)
        -> Distancia a la que los golpes logran realizar contacto (booleano)
        -> Vida (flotante, normalizado)
        -> Vida del enemigo (flotante, normalizado)
        -> Acciones restringidas debido a que el agente acaba de recibir un golpe (booleano)
        -> Ataques restringidos porque estan en tiempo de recuperación (booleano)
        -> Acciones restringidas debido a que el agente esta en el aire (booleano)
        -> El agente enemigo esta bloqueando (booleano)
    Acciones:
        -> Branch 1:
            <- Caminar(Dos acciones) 
            <- Impulsos(Dos acciones)
            <- Agacharse(Una accion)
            <- No hacer nada(Una acción)
        -> Branch 2:
            <- Saltar
            <- Golpes(Cuatro acciones)
            <- Patadas(Cuatro acciones)
            <- Bloquear(Una accion)
            <- No hacer nada(Una acción)
    Recompensas:
        -> -10 / MaxSteps cada step (Un step es cada iteracion del episodio de entrenamiento)
        -> +Daño aplicado al agente enemigo * (0.1)
        -> -Daño recibido del agente enemigo * (0.1)
        -> +0.5 por bloquear un ataque
        -> +0.05 por aplicar daño mientras el enemigo esta cubierto
        -> +20 por ganar

Agente 1 (Modelo 1)

A partir de este modelo, el juego esta completamente probado y ya no hay cosas que puedan resultar en bugs o en injusticias que interfieran en el entrenamiento del agente inteligente

Caracteristicas.
    Observaciones:
        -> Distancia entre los dos agentes normalizada (float)
        -> Vida del agente normalizada (float)
        -> Vida del agente enemigo normalizada (float)
        -> Ataques en enfriamiento (bool)
        -> Resistencia del agente normalizada (float)
        -> Muy cerca del agente enemigo (bool)
        -> Cantidad de ataques encadenados normalizada (float)
        -> Orientación del personaje (flotante que vale 1 y -1)
        -> Vector de estados del agente, son 16 estados en total (vector de bool)
        -> Vector de estados del enemigo, son 16 estados en total (vector de bool)
    Acciones:
        -> Branch 1:
            <- Quieto
            <- Moverse con velocidad positiva
            <- Moverse con velocidad negativa
            <- Saltar
            <- Golpe debil
            <- Golpe ligero
            <- Golpe fuerte
            <- Golpe especial
            <- Patada debil
            <- Patada ligera
            <- Patada fuerte
            <- Patada especial
    Recompensas:
        -0.000001 cada 0.2 segundos aproximadamente
        +1 por golpear al agente enemigo
        -1 por ser golpeado por el agente enemigo
        +1 por bloquear un ataque del agente enemigo
        -0.0001 por atacar a un agente bloqueando
        +10 por incapacitar al agente enemigo
        -10 por dejarse incapacitar
        -0.01 por realizar ataques al aire
        +100 por ganar
        -100 por perder

    Ajustes adicionales:
        Cada que un se inicia un episodio, los agentes apareceran a 5 unidades de distancia
        La vida de los personajes será de 50 (la mitad de lo que un juego real es)
        Aproximadamente cada episodio durará 1000 steps
        Las instancias que se usan para entrenar a los agentes en paralelo son nueve

Agente 1.1 (Modelo 1.1)
Las recompensas del anterior modelo no funcionaron correctamente porque llega un punto en que los dos agentes dejan de golpearse, ni siquiera intentan acercarse a su objetivo
entonces se modifica la recompensa de la siguiente manera:
    Recompensas:
            +1 por golpear al agente enemigo
            -1 por ser golpeado por el agente enemigo
            +1 por bloquear un ataque del agente enemigo
            -0.0001 por atacar a un agente bloqueando
            +10 por incapacitar al agente enemigo
            -10 por dejarse incapacitar
            -0.01 por realizar ataques al aire
            +1000 por ganar y si el agente llevaba recompensa positiva, esta se le suma
            -1000 por perder y si el agente llevaba recompensa negativa esta se le suma

Nota: Este modelo me dio aentender que las dos últimas recompensas hechas de esta manera afectan negativamente el aprendizaje del agente debido a que el agente
comprende que puede hacer cualuqier cosa y que al final del episodio debe acercarse y matar a su enemigo (o eso intenta)

Agente 1.2 (Modelo 1.2)
Falta de documentar

Agente 1.3 (Modelo 1.3)
Aún no consigo que los personajes se acerquen para golpear, asi que voy a intentar recompensar a los agentes cuando esten cerca uno del otro,
ademas, los movimientos del personaje se estan confundiendo porque una misma acción lo mueve positiva y negativamente entonces para este modelo
cambie las acciones de caminar y retroceder, ahora se pueden ver más como Caminar = velocidad positiva y Retroceder = velocidad negativa
    Observaciones:
        -> Distancia entre los dos agentes normalizada (float)
        -> Vida del agente normalizada (float)
        -> Vida del agente enemigo normalizada (float)
        -> Ataques en enfriamiento (bool)
        -> Resistencia del agente normalizada (float)
        -> Muy cerca del agente enemigo (bool)
        -> Cantidad de ataques encadenados normalizada (float)
        |-> Velocidad del agente (normalizada) 
        -> Vector de estados del agente, son 16 estados en total (vector de bool)
        -> Vector de estados del enemigo, son 16 estados en total (vector de bool)

    Recompensas:
            +1 por golpear al agente enemigo
            -1 por ser golpeado por el agente enemigo
            +1 por bloquear un ataque del agente enemigo
            +10 por incapacitar al agente enemigo
            -10 por dejarse incapacitar
            -0.01 por realizar ataques al aire
            -0.01 por cada 50 steps que el agente no le haya quitado vida al enemigo
            -0.001 por cada 10 steps que el agente no se haya acercado a su enemigo 
            +50 por ganar
            -50 por perder

    Ajustes adicionales:
        Cada que un se inicia un episodio, los agentes apareceran a 5 unidades de distancia
        La vida de los personajes será de 30
        Cada episodio durará 1000 steps
        Las instancias que se usan para entrenar a los agentes en paralelo son nueve
        TakeActionsBetweenDecisions esta desactivado
        DecisionPeriod = 10
        resitencia de los personajes en 20

Agente 1.3.1 (Modelo 1.3.1)
Parece que las cosas mejoran pero se estanca en un óptimo local debido a que es mas rentable no hacer nada
que hacer alguna acción

Agente 2.0 (Modelo 2.0)
    Acciones:
        -> Branch 1:
            <- No hacer nada de esta branch
            <- Moverse con velocidad positiva
            <- Moverse con velocidad negativa
        -> Branch 2:
            <- No hacer nada de esta branch
            <- Saltar
            <- Golpe debil
            <- Golpe ligero
            <- Golpe fuerte
            <- Golpe especial
            <- Patada debil
            <- Patada ligera
            <- Patada fuerte
            <- Patada especial
    Observaciones:
        -> Distancia entre los dos agentes normalizada (float)
        -> Vida del agente normalizada (float)
        -> Vida del agente enemigo normalizada (float)
        -> Ataques en enfriamiento (bool)
        -> Resistencia del agente normalizada (float)
        -> Muy cerca del agente enemigo (bool)
        -> Cantidad de ataques encadenados normalizada (float)
        -> Velocidad del agente (normalizada) 
        -> Vector de estados del agente, son 16 estados
        -> Vector de estados del enemigo, son 16 estados
    Recompensas:
            +1 por golpear al agente enemigo
            -1 por ser golpeado por el agente enemigo
            +1 por bloquear un ataque del agente enemigo
            +10 por incapacitar al agente enemigo
            -10 por dejarse incapacitar
        |   -0.01 += (0.001) por cada 50 steps que el agente no le haya quitado vida al enemigo (se resetea el valor a -0.01 cada vez que el agete daña a su enemigo)
            +50 por ganar
            -50 por perder
    Ajustes adicionales:
        Cada que un se inicia un episodio, los agentes apareceran a 5 unidades de distancia
        La vida de los personajes será de 30
        Cada episodio durará 1000 steps
        Las instancias que se usan para entrenar a los agentes en paralelo son nueve
        TakeActionsBetweenDecisions esta desactivado
        DecisionPeriod = 10
        resitencia de los personajes en 20
        EndEpisode siempre y cuando uno de los dos enemigos tenga mas vida que el otro sino EpisodeInterrupted()

Nota: Con este modelo me doy cuenta que tal vez la parte del bloqueo es demasiado para el personaje pensar en remplantearlo pero solo para la IA

Agente 2.1 (Modelo 2.1)
    Todo igual a excepcion de la recompensa por bloquear, ya no existe

Agente 2.2 (Modelo 2.2)
    Recompensas:
            +1 por golpear al agente enemigo
            -1 por ser golpeado por el agente enemigo
            +1 por bloquear un ataque del agente enemigo
            +10 por incapacitar al agente enemigo
            -10 por dejarse incapacitar
        |   -0.01 += (0.005) por cada 10 steps que el agente no le haya quitado vida al enemigo (se resetea el valor a -0.01 cada vez que el agete daña a su enemigo)
        |   +10 por ganar
            -10 por perder

    Ajustes adicionales:
        Cada que un se inicia un episodio, los agentes apareceran a 5 unidades de distancia
        La vida de los personajes será de 30
        Cada episodio durará 1000 steps
        Las instancias que se usan para entrenar a los agentes en paralelo son nueve
        TakeActionsBetweenDecisions esta desactivado
        DecisionPeriod = 10
        resitencia de los personajes en 20
        EndEpisode siempre y cuando uno de los dos enemigos tenga mas vida que el otro sino EpisodeInterrupted()
        Las acciones se esconden dependiendo del estado

Agente 3.0 (Modelo 3.0)
Agregar bloqueo como acción, en vez de dejar que se detone sola

Agente 4.0

Vamos a ir por partes con este agente usando uso del concepto "Curriculum Learning" debido a que el personaje no esta aprendiendo como me gustaría que aprendiera

Fase 1: 
    Observaciones:
     |  -> Enemigo a la izquierda o a la derecha
        -> Distancia entre los dos agentes normalizada (float)
        -> Vida del agente normalizada (float)
        -> Vida del agente enemigo normalizada (float)
        -> Ataques en enfriamiento (bool)
        -> Resistencia del agente normalizada (float)
        -> Muy cerca del agente enemigo (bool)
        -> Cantidad de ataques encadenados normalizada (float)
        -> Velocidad del agente (normalizada) 
        -> Vector de estados del agente, son 16 estados en total (vector de bool)
        -> Vector de estados del enemigo, son 16 estados en total (vector de bool)
    Acciones:
        -> Branch 1:
            <- No hacer nada de esta branch
            <- Moverse con velocidad positiva
            <- Moverse con velocidad negativa
        -> Branch 2:
            <- No hacer nada de esta branch
            <- Saltar
            <- Golpe debil
            <- Golpe ligero
            <- Golpe fuerte
            <- Golpe especial
            <- Patada debil
            <- Patada ligera
            <- Patada fuerte
            <- Patada especial
    Recompensas:
        +1 por alcanzar al enemigo

    Ajustes Adicionales:
        Cada que un se inicia un episodio, los agentes apareceran a 5 unidades de distancia
        Cada episodio durará 250 steps
        Las instancias que se usan para entrenar a los agentes en paralelo son nueve
        TakeActionsBetweenDecisions esta desactivado
        DecisionPeriod = 10
        resitencia de los personajes en 20
        Las acciones se esconden dependiendo del estado
        Un solo agente entrenandose, no selfPlay

Fase 2:
    Ajustes Adicionales:
      | Cada que un se inicia un episodio, los agentes apareceran a 10 unidades de distancia
        Cada episodio durará 250 steps
        Las instancias que se usan para entrenar a los agentes en paralelo son nueve
        TakeActionsBetweenDecisions esta desactivado
        DecisionPeriod = 10
        resitencia de los personajes en 20
        Las acciones se esconden dependiendo del estado
        Un solo agente entrenandose, no selfPlay

Fase 3:
La recompensa se aplica solo si esta en el suelo

